{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity-Based Learning: Self-Supervised and Supervised "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ N =K$$\n",
    "$$\n",
    "p_i = p_{\\text{aug}}(\\cdot | \\text{img}_i), \\quad \\text{for } i = 1, 2, \\dots, K\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Classes:**\n",
    "\n",
    "$$\n",
    "C = N\n",
    "$$\n",
    "\n",
    "**Logits Vector $\\mathbf{z} $:**\n",
    "$$\n",
    "z_j = f(x_i)^\\top f(y_j), \\quad \\text{for } j = 1, 2, \\dots, K\n",
    "$$\n",
    "\n",
    "**Target Class：**\n",
    "For ${(x_i,y_i)}$, target class should be $i$,  $\\text{for } i = 1, 2, \\dots, K$\n",
    "\n",
    "**Range of Logits Entries:** Given the $f(x_i)$ outputs a unit vector,  the $f(x_i)^\\top f(y_j)$ maps the cosine angle between two unit vectors\n",
    "\n",
    "$$\n",
    "f(x_i)^\\top f(y_j) \\in [-1, 1]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Euclidean Distance VS Dot Product:**\n",
    "\n",
    "  $$\n",
    "  \\| u - v \\|^2 = (u-v)^\\top (u-v)=  \\| u \\|^2 + \\| v \\|^2 - 2u^\\top v\n",
    "  $$\n",
    "\n",
    "**Relation to Cosine Similarity:**\n",
    "\n",
    "Given $ \\| u \\| = \\| v \\| = 1 $:\n",
    "\n",
    "  $$\n",
    "  \\| u - v \\|^2 = 2(1 - u^\\top v)\n",
    "  $$\n",
    "\n",
    "  Since $ u^\\top v = \\cos \\theta \\| u \\|\\| v \\|  $:\n",
    "\n",
    "  $$\n",
    "  \\| u - v \\|^2 = 2(1 - \\cos \\theta)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ``Yes`` \\\n",
    "(b)  ``No`` \\\n",
    "(c)  ``Yes`` \\\n",
    "(d) ``No``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$diameter_i(f )$ represents the maximum Euclidean distance between feature vectors within the same set $S_i$.  $margin_{i→j} (f )$ is minimum Euclidean distance between feature vectors across different sets $S_i$,$S_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``ii``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i)``smaller``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii)``larger``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii)(A) The collection of invariance sets is {${S_i}$}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii)(B) The extremization in (iii)A potentially leads to non-uniform placement, as points are clustered around constants representing each $S_i$, unlike the roughly uniform distribution sought in the Tammes problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``No``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $p_i$ and $p_j$ have overlapping support, an optimal encoder $f$ will place feature vectors from the overlapping region close together, potentially causing misclassification for pairs ($x_i$,$y_j$) in this region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``No``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. As we relax the constraints, the output of $f(x_i)$ may have larger magnitude than 1. Let $f(x_i) ^\\top f(y_i)  \\rightarrow \\alpha^2 f(x_i)^\\top f(y_i) $. As $\\alpha$  becomes very large, the numerator grows exponentially faster than denominator for $i\\neq j $\n",
    "$$ - \\log \\left( \\frac{e^{\\alpha^2 f(x_i)^\\top f(y_i)}}{\\sum_{j=1}^N e^{\\alpha^2 f(x_i)^\\top f(y_j)}} \\right) \\rightarrow - \\log(1) = 0 $$\n",
    "\n",
    "As for numerical stability, scaling up the outputs can cause numerical overflow in exponential computations, for instance the encoder learns an extreme 'shortcut', scaling $f(x_i) ^\\top f(y_i)$ even to infinity to minimize the contrastive loss, leading to instability during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can difine $f_1(x)$ as $g(x)$ and use $f_2(x)$ as one-hot encoding as class labels, which maps the indexing to product of two vectors.\n",
    "$$ N = C $$\n",
    "$$p_i = \\text{Uniform}\\left( \\{ x_k \\mid y_k = i \\} \\right)$$\n",
    "$$d = C$$\n",
    "$$ X_1 = X $$\n",
    "$$ X_2 = \\{1, 2, \\dots, C\\} $$\n",
    "$$ f_1(x) = g(x) \\in \\mathbb{R}^C $$\n",
    "$$ f_2(c) = e_c \\in \\mathbb{R}^C \\text{(one-hot encoding of class label)}$$ \n",
    "$$ L_{\\text{dual-enc-contr}}(f_1, f_2) = L_{\\text{xce}}(g) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the mapping defined above,  we have $f_1(x)^\\top f_2(y_j) = g(x)^\\top e_{y_j} = g(x)[y_j]$ \\\n",
    "Therefore $$\\hat{y}_{\\text{Contr}} = \\arg\\max_{j \\in \\{1, 2, \\dots, C\\}} g(x)[j]$$\n",
    "$$\\hat{y}_{\\text{CE}} = \\hat{y}_{\\text{Contr}}$$\n",
    "$$\\mathbf{1} \\{\\hat{y}_{\\text{CE}} = y\\} = \\mathbf{1} \\{\\hat{y}_{\\text{Contr}} = y\\}$$\n",
    "$$\\text{Accuracy}_{\\text{CE}} = \\text{Accuracy}_{\\text{Contr}}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
