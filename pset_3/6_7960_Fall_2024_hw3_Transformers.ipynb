{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "47cd59de",
      "metadata": {
        "id": "47cd59de"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple, Union, Optional, List\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "baab5229",
      "metadata": {
        "id": "baab5229"
      },
      "outputs": [],
      "source": [
        "# a utility for calculating running average\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.num = 0\n",
        "        self.tot = 0\n",
        "\n",
        "    def update(self, val: float, sz: float):\n",
        "        self.num += val * sz\n",
        "        self.tot += sz\n",
        "\n",
        "    def calculate(self) -> float:\n",
        "        return self.num / self.tot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eb743ee",
      "metadata": {
        "id": "6eb743ee"
      },
      "source": [
        "# Problem 2: Implement a Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6cffe0a",
      "metadata": {
        "id": "e6cffe0a"
      },
      "source": [
        "## Part 2.A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "29fc4254",
      "metadata": {
        "id": "29fc4254"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, dim: int, n_hidden: int):\n",
        "        # dim: the dimension of the input\n",
        "        # n_hidden: the dimension of the keys, queries, and values\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_K = nn.Linear(dim, n_hidden)  # W_K weight matrix\n",
        "        self.W_Q = nn.Linear(dim, n_hidden)  # W_Q weight matrix\n",
        "        self.W_V = nn.Linear(dim, n_hidden)  # W_V weight matrix\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # x                the inputs. shape: (B x T x dim)\n",
        "        # attn_mask        an attention mask. If None, ignore. If not None, then mask[b, i, j]\n",
        "        #                  contains 1 if (in batch b) token i should attend on token j and 0\n",
        "        #                  otherwise. shape: (B x T x T)\n",
        "        #\n",
        "        # Outputs:\n",
        "        # attn_output      the output of performing self-attention on x. shape: (Batch x Num_tokens x n_hidden)\n",
        "        # alpha            the attention weights (after softmax). shape: (B x T x T)\n",
        "        #\n",
        "\n",
        "        out, alpha = None, None\n",
        "        # TODO: Compute self attention on x.\n",
        "        #       (1) First project x to the query Q, key K, value V.\n",
        "        #       (2) Then compute the attention weights alpha as:\n",
        "        #                  alpha = softmax(QK^T/sqrt(n_hidden))\n",
        "        #           Make sure to take into account attn_mask such that token i does not attend on token\n",
        "        #           j if attn_mask[b, i, j] == 0. (Hint, in such a case, what value should you set the weight\n",
        "        #           to before the softmax so that after the softmax the value is 0?)\n",
        "        #       (3) The output is a linear combination of the values (weighted by the alphas):\n",
        "        #                  out = alpha V\n",
        "        #       (4) return the output and the alpha after the softmax\n",
        "\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        query = self.W_Q(x)\n",
        "        key = self.W_K(x)\n",
        "        value = self.W_V(x)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.n_hidden**0.5\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.to(scores.device)\n",
        "            scores = scores.masked_fill(attn_mask == 0, float(\"-inf\"))\n",
        "        alpha = torch.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(alpha, value)\n",
        "\n",
        "        # ======= Answer  END ========\n",
        "\n",
        "        return attn_output, alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d9cb9ad",
      "metadata": {
        "id": "7d9cb9ad"
      },
      "source": [
        "## Part 2.B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ddd56734",
      "metadata": {
        "id": "ddd56734"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, dim: int, n_hidden: int, num_heads: int):\n",
        "        # dim: the dimension of the input\n",
        "        # n_hidden: the hidden dimenstion for the attention layer\n",
        "        # num_heads: the number of attention heads\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: set up your parameters for multi-head attention. You should initialize\n",
        "        #       num_heads attention heads (see nn.ModuleList) as well as a linear layer\n",
        "        #       that projects the concatenated outputs of each head into dim\n",
        "        #       (what size should this linear layer be?)\n",
        "\n",
        "        # ======= Answer START ========\n",
        "        self.num_heads = num_heads\n",
        "        self.n_hidden = n_hidden\n",
        "        self.attn_heads = nn.ModuleList(\n",
        "            [AttentionHead(dim=dim, n_hidden=n_hidden) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.project_layer = nn.LazyLinear(num_heads * n_hidden, dim)\n",
        "\n",
        "        # ======= Answer  END ========\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # x                the inputs. shape: (B x T x dim)\n",
        "        # attn_mask        an attention mask. If None, ignore. If not None, then mask[b, i, j]\n",
        "        #                  contains 1 if (in batch b) token i should attend on token j and 0\n",
        "        #                  otherwise. shape: (B x T x T)\n",
        "        #\n",
        "        # Outputs:\n",
        "        # attn_output      the output of performing multi-headed self-attention on x.\n",
        "        #                  shape: (B x T x dim)\n",
        "        # attn_alphas      the attention weights of each of the attention heads.\n",
        "        #                  shape: (B x Num_heads x T x T)\n",
        "\n",
        "        attn_output, attn_alphas = None, None\n",
        "\n",
        "        # TODO: Compute multi-headed attention. Loop through each of your attention heads\n",
        "        #       and collect the outputs. Concatenate them together along the hidden dimension,\n",
        "        #       and then project them back into the output dimension (dim). Return both\n",
        "        #       the final attention outputs as well as the alphas from each head.\n",
        "\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        outputs = []\n",
        "        alphas = []\n",
        "        for attn_layer in self.attn_heads:\n",
        "            output, alpha = attn_layer(x=x, attn_mask=attn_mask)\n",
        "            outputs.append(output)  # output (B, T, n_hidden)\n",
        "            alphas.append(alpha)  # alpha (B, T, T)\n",
        "\n",
        "        attn_output = torch.cat(outputs, dim=-1)  # (B,T,Num_heads*n_hidden)\n",
        "        attn_alphas = torch.stack(alphas, dim=1)  # (B,Num_heads,T,T)\n",
        "        attn_output = self.project_layer(attn_output)  # (B,Num_heads,dim)\n",
        "\n",
        "        # ======= Answer END ========\n",
        "        return attn_output, attn_alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8012740",
      "metadata": {
        "id": "a8012740"
      },
      "source": [
        "## Part 2.C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8531bc02",
      "metadata": {
        "id": "8531bc02"
      },
      "outputs": [],
      "source": [
        "# these are already implemented for you!\n",
        "\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, dim: int, n_hidden: int):\n",
        "\n",
        "\n",
        "        # dim       the dimension of the input\n",
        "\n",
        "        # n_hidden  the width of the linear layer\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, n_hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(n_hidden, dim),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        # x         the input. shape: (B x T x dim)\n",
        "\n",
        "\n",
        "        # Outputs:\n",
        "\n",
        "        # out       the output of the feed-forward network: (B x T x dim)\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class AttentionResidual(nn.Module):\n",
        "    def __init__(self, dim: int, attn_dim: int, mlp_dim: int, num_heads: int):\n",
        "\n",
        "\n",
        "        # dim       the dimension of the input\n",
        "\n",
        "        # attn_dim  the hidden dimension of the attention layer\n",
        "\n",
        "\n",
        "        # mlp_dim   the hidden layer of the FFN\n",
        "\n",
        "        # num_heads the number of heads in the attention layer\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.attn = MultiHeadedAttention(dim, attn_dim, num_heads)\n",
        "\n",
        "\n",
        "        self.ffn = FFN(dim, mlp_dim)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, attn_mask: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        # x                the inputs. shape: (B x T x dim)\n",
        "\n",
        "\n",
        "        # attn_mask        an attention mask. If None, ignore. If not None, then mask[b, i, j]\n",
        "\n",
        "        #                  contains 1 if (in batch b) token i should attend on token j and 0\n",
        "\n",
        "        #                  otherwise. shape: (B x T x T)\n",
        "\n",
        "        #\n",
        "\n",
        "        # Outputs:\n",
        "\n",
        "        # attn_output      shape: (B x T x dim)\n",
        "\n",
        "        # attn_alphas      the attention weights of each of the attention heads.\n",
        "\n",
        "        #                  shape: (B x Num_heads x T x T)\n",
        "\n",
        "\n",
        "        attn_out, alphas = self.attn(x=x, attn_mask=attn_mask)\n",
        "\n",
        "        x = attn_out + x\n",
        "\n",
        "        x = self.ffn(x) + x\n",
        "        return x, alphas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "02c21aea",
      "metadata": {
        "id": "02c21aea"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self, dim: int, attn_dim: int, mlp_dim: int, num_heads: int, num_layers: int\n",
        "    ):\n",
        "        # dim       the dimension of the input\n",
        "        # attn_dim  the hidden dimension of the attention layer\n",
        "        # mlp_dim   the hidden layer of the FFN\n",
        "        # num_heads the number of heads in the attention layer\n",
        "        # num_layers the number of attention layers.\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: set up the parameters for the transformer!\n",
        "        #       You should set up num_layers of AttentionResiduals\n",
        "        #       nn.ModuleList will be helpful here.\n",
        "\n",
        "        # ======= Answer START ========\n",
        "        self.attn_net = nn.ModuleList(\n",
        "            [\n",
        "                AttentionResidual(\n",
        "                    dim=dim, attn_dim=attn_dim, mlp_dim=mlp_dim, num_heads=num_heads\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # ======= Answer END ========\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, attn_mask: torch.Tensor, return_attn=False\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        # x                the inputs. shape: (B x T x dim)\n",
        "        # attn_mask        an attention mask. Pass this to each of the AttentionResidual layers!\n",
        "        #                  shape: (B x T x T)\n",
        "        #\n",
        "        # Outputs:\n",
        "        # attn_output      shape: (B x T x dim)\n",
        "        # attn_alphas      If return_attn is False, return None. Otherwise return the attention weights\n",
        "        #                  of each of each of the attention heads for each of the layers.\n",
        "        #                  shape: (B x Num_layers x Num_heads x T x T)\n",
        "\n",
        "        output, collected_attns = None, None\n",
        "\n",
        "        # TODO: Implement the transformer forward pass! Pass the input successively through each of the\n",
        "        # AttentionResidual layers. If return_attn is True, collect the alphas along the way.\n",
        "\n",
        "        # ======= Answer START ========\n",
        "        attn_alphas = []\n",
        "        for attn_res in self.attn_net:\n",
        "            x, alphas = attn_res(x=x, attn_mask=attn_mask)\n",
        "            attn_alphas.append(alphas)\n",
        "        output = x\n",
        "        if return_attn:\n",
        "            collected_attns = torch.stack(attn_alphas, dim=1)\n",
        "\n",
        "        # ======= Answer END ========\n",
        "\n",
        "        return output, collected_attns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "643d379b",
      "metadata": {
        "id": "643d379b"
      },
      "source": [
        "Test your transformer implementation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9aaa156e",
      "metadata": {
        "id": "9aaa156e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case 1\n",
            "Test Case 2\n",
            "Test Case 3\n",
            "Attention mask pattern tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 1., 1.]], device='cuda:0')\n",
            "Alpha pattern tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.4510, 0.5490, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.4537, 0.5463,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.5596, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.4068, 0.5932, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7301, 0.2699]],\n",
            "       device='cuda:0')\n",
            "Test Case 4\n"
          ]
        }
      ],
      "source": [
        "def perform_transformer_test_cases():\n",
        "    num_tokens = 100\n",
        "    batch_size = 10\n",
        "    dim = 64\n",
        "    num_layers = 4\n",
        "    num_heads = 2\n",
        "    dummy_model = Transformer(\n",
        "        dim=dim, attn_dim=32, mlp_dim=dim, num_heads=num_heads, num_layers=num_layers\n",
        "    ).cuda()\n",
        "\n",
        "    inp = torch.randn(batch_size, num_tokens, dim).cuda()\n",
        "\n",
        "    # test case 1 regular forward pass\n",
        "    print(\"Test Case 1\")\n",
        "    with torch.no_grad():\n",
        "        output, alpha = dummy_model(inp, attn_mask=None)\n",
        "        assert alpha is None\n",
        "        assert output.shape == (\n",
        "            batch_size,\n",
        "            num_tokens,\n",
        "            dim,\n",
        "        ), f\"wrong output shape {output.shape}\"\n",
        "\n",
        "    # test case 2 collect attentions\n",
        "    print(\"Test Case 2\")\n",
        "    with torch.no_grad():\n",
        "        output, alpha = dummy_model(inp, attn_mask=None, return_attn=True)\n",
        "        assert output.shape == (\n",
        "            batch_size,\n",
        "            num_tokens,\n",
        "            dim,\n",
        "        ), f\"wrong output shape {output.shape}\"\n",
        "        assert alpha.shape == (\n",
        "            batch_size,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            num_tokens,\n",
        "            num_tokens,\n",
        "        ), f\"wrong alpha shape {alpha.shape}\"\n",
        "\n",
        "    print(\"Test Case 3\")\n",
        "    # test case 3 with attention mask\n",
        "    attn_mask = torch.zeros(batch_size, num_tokens, num_tokens).cuda()\n",
        "    attn_mask[:, torch.arange(num_tokens), torch.arange(num_tokens)] = 1\n",
        "    attn_mask[:, torch.arange(num_tokens)[1:], torch.arange(num_tokens)[:-1]] = 1\n",
        "    with torch.no_grad():\n",
        "        output, alpha = dummy_model(inp, attn_mask=attn_mask, return_attn=True)\n",
        "        print(\"Attention mask pattern\", attn_mask[0])\n",
        "        print(\"Alpha pattern\", alpha[0, 0, 0])\n",
        "        assert torch.all(alpha.permute(1, 2, 0, 3, 4)[:, :, attn_mask == 0] == 0).item()\n",
        "\n",
        "    print(\"Test Case 4\")\n",
        "    # test case 4 creates a causal mask where each token can only attend to previous tokens and itself\n",
        "    causal_mask = (\n",
        "        torch.tril(torch.ones(num_tokens, num_tokens))\n",
        "        .unsqueeze(0)\n",
        "        .repeat(batch_size, 1, 1)\n",
        "    )  # Shape: (B, T, T)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output, alpha = dummy_model(inp, attn_mask=causal_mask, return_attn=True)\n",
        "        # Verify the causal mask\n",
        "        for b in range(batch_size):\n",
        "            for l in range(num_layers):\n",
        "                for h in range(num_heads):\n",
        "                    attn_weights = alpha[b, l, h]  # Shape: (T, T)\n",
        "                    # Positions where j > i should have zero attention weights\n",
        "                    # We can create a boolean mask for j > i\n",
        "                    future_mask = torch.triu(\n",
        "                        torch.ones(num_tokens, num_tokens), diagonal=1\n",
        "                    ).bool()  # Shape: (T, T)\n",
        "                    # Extract attention weights for future positions\n",
        "                    future_attn = attn_weights[future_mask]\n",
        "                    # Assert that these weights are close to zero\n",
        "                    assert torch.all(\n",
        "                        future_attn < 1e-6\n",
        "                    ), f\"Causal mask violated in batch {b}, layer {l}, head {h}\"\n",
        "\n",
        "\n",
        "perform_transformer_test_cases()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5afc7e9b",
      "metadata": {
        "id": "5afc7e9b"
      },
      "source": [
        "## Problem 3: Vision Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67b725d5",
      "metadata": {
        "id": "67b725d5"
      },
      "source": [
        "## Part 3.A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dc4ea76",
      "metadata": {
        "id": "0dc4ea76"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Image to Patch Embedding\"\"\"\n",
        "\n",
        "    def __init__(self, img_size: int, patch_size: int, nin: int, nout: int):\n",
        "        # img_size       the width and height of the image. you can assume that\n",
        "        #                the images will be square\n",
        "        # patch_size     the width of each square patch. You can assume that\n",
        "        #                img_size is divisible by patch_size\n",
        "        # nin            the number of input channels\n",
        "        # nout           the number of output channels\n",
        "\n",
        "        super().__init__()\n",
        "        assert img_size % patch_size == 0\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # TODO Set up parameters for the Patch Embedding\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        # ======= Answer END ========\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x        the input image. shape: (B, nin, Height, Width)\n",
        "        #\n",
        "        # Output\n",
        "        # out      the patch embeddings for the input. shape: (B, num_patches, nout)\n",
        "\n",
        "        # TODO: Implement the patch embedding. You want to split up the image into\n",
        "        # square patches of the given patch size. Then each patch_size x patch_size\n",
        "        # square should be linearly projected into an embedding of size nout.\n",
        "        #\n",
        "        # Hint: Take a look at nn.Conv2d. How can this be used to perform the\n",
        "        #       patch embedding?\n",
        "        out = None\n",
        "\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        # ======= Answer END ========\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57d32978",
      "metadata": {
        "id": "57d32978"
      },
      "source": [
        "## Part 3.B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df3d8a7a",
      "metadata": {
        "id": "df3d8a7a"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels: int,\n",
        "        nout: int,\n",
        "        img_size: int,\n",
        "        patch_size: int,\n",
        "        dim: int,\n",
        "        attn_dim: int,\n",
        "        mlp_dim: int,\n",
        "        num_heads: int,\n",
        "        num_layers: int,\n",
        "    ):\n",
        "        # n_channels       number of input image channels\n",
        "        # nout             desired output dimension\n",
        "        # img_size         width of the square image\n",
        "        # patch_size       width of the square patch\n",
        "        # dim              embedding dimension\n",
        "        # attn_dim         the hidden dimension of the attention layer\n",
        "        # mlp_dim          the hidden layer dimension of the FFN\n",
        "        # num_heads        the number of heads in the attention layer\n",
        "        # num_layers       the number of attention layers.\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, nin=n_channels, nout=dim\n",
        "        )\n",
        "        self.pos_E = nn.Embedding(\n",
        "            (img_size // patch_size) ** 2, dim\n",
        "        )  # positional embedding matrix\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))  # learned class embedding\n",
        "        self.transformer = Transformer(\n",
        "            dim=dim,\n",
        "            attn_dim=attn_dim,\n",
        "            mlp_dim=mlp_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, nout))\n",
        "\n",
        "    def forward(\n",
        "        self, img: torch.Tensor, return_attn=False\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        # img          the input image. shape: (B, nin, img_size, img_size)\n",
        "        # return_attn  whether to return the attention alphas\n",
        "        #\n",
        "        # Outputs\n",
        "        # out          the output of the vision transformer. shape: (B, nout)\n",
        "        # alphas       the attention weights for all heads and layers. None if return_attn is False, otherwise\n",
        "        #              shape: (B, num_layers, num_heads, num_patches + 1, num_patches + 1)\n",
        "\n",
        "        # generate embeddings\n",
        "        embs = self.patch_embed(img)  # patch embedding\n",
        "        B, T, _ = embs.shape\n",
        "        pos_ids = torch.arange(T).expand(B, -1).to(embs.device)\n",
        "        embs += self.pos_E(pos_ids)  # positional embedding\n",
        "\n",
        "        cls_token = self.cls_token.expand(len(embs), -1, -1)\n",
        "        x = torch.cat([cls_token, embs], dim=1)\n",
        "\n",
        "        x, alphas = self.transformer(x, attn_mask=None, return_attn=return_attn)\n",
        "        out = self.head(x)[:, 0]\n",
        "        return out, alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bcc81df",
      "metadata": {
        "id": "8bcc81df"
      },
      "source": [
        "## Part 3.C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1107044",
      "metadata": {
        "id": "e1107044"
      },
      "outputs": [],
      "source": [
        "# set up the dataset and dataloader\n",
        "\n",
        "MEAN = [0.4914, 0.4822, 0.4465]\n",
        "STD = [0.2470, 0.2435, 0.2616]\n",
        "img_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=MEAN, std=STD),\n",
        "    ]\n",
        ")\n",
        "inv_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Normalize(mean=[0.0, 0.0, 0.0], std=1 / np.array(STD)),\n",
        "        transforms.Normalize(mean=-np.array(MEAN), std=[1.0, 1.0, 1.0]),\n",
        "        transforms.ToPILImage(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    train=True, root=\"data\", transform=img_transform, download=True\n",
        ")\n",
        "val_dataset = torchvision.datasets.CIFAR10(\n",
        "    train=False, root=\"data\", transform=img_transform\n",
        ")\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=256, shuffle=True, num_workers=10\n",
        ")\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=256, shuffle=False, num_workers=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2540d4b3",
      "metadata": {
        "id": "2540d4b3"
      },
      "outputs": [],
      "source": [
        "# set up the model and optimizer\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "model = VisionTransformer(\n",
        "    n_channels=3,\n",
        "    nout=10,\n",
        "    img_size=32,\n",
        "    patch_size=4,\n",
        "    dim=128,\n",
        "    attn_dim=64,\n",
        "    mlp_dim=128,\n",
        "    num_heads=3,\n",
        "    num_layers=6,\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147e1bc8",
      "metadata": {
        "id": "147e1bc8"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "def evaluate_cifar_model(model, criterion, val_loader):\n",
        "    is_train = model.training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss_meter, acc_meter = AverageMeter(), AverageMeter()\n",
        "        for img, labels in val_loader:\n",
        "            # move all img, labels to device (cuda)\n",
        "            img = img.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs, _ = model(img)\n",
        "            loss_meter.update(criterion(outputs, labels).item(), len(img))\n",
        "            acc = (outputs.argmax(-1) == labels).float().mean().item()\n",
        "            acc_meter.update(acc, len(img))\n",
        "    model.train(is_train)\n",
        "    return loss_meter.calculate(), acc_meter.calculate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d69183ce",
      "metadata": {
        "id": "d69183ce"
      },
      "outputs": [],
      "source": [
        "# Time Estimate: less than 5 minutes on T4 GPU\n",
        "# train the model\n",
        "import tqdm\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):  #\n",
        "    loss_meter = AverageMeter()\n",
        "    acc_meter = AverageMeter()\n",
        "    for img, labels in tqdm.tqdm(train_dataloader):\n",
        "        img, labels = img.cuda(), labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs, _ = model(img)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_meter.update(loss.item(), len(img))\n",
        "        acc = (outputs.argmax(-1) == labels).float().mean().item()\n",
        "        acc_meter.update(acc, len(img))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "    print(\n",
        "        f\"Train Epoch: {epoch}, Loss: {loss_meter.calculate()}, Acc: {acc_meter.calculate()}\"\n",
        "    )\n",
        "    if epoch % 10 == 0:\n",
        "        val_loss, val_acc = evaluate_cifar_model(model, criterion, val_dataloader)\n",
        "        print(f\"Val Epoch: {epoch}, Loss: {val_loss}, Acc: {val_acc}\")\n",
        "\n",
        "val_loss, val_acc = evaluate_cifar_model(model, criterion, val_dataloader)\n",
        "print(f\"Val Epoch: {epoch}, Loss: {val_loss}, Acc: {val_acc}\")\n",
        "print(\"Finished Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22a64f80",
      "metadata": {
        "id": "22a64f80"
      },
      "source": [
        "# Part 3.D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d90ecba6",
      "metadata": {
        "id": "d90ecba6"
      },
      "outputs": [],
      "source": [
        "for val_batch in val_dataloader:\n",
        "    break\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    img, labels = val_batch\n",
        "    img = img.cuda()\n",
        "    outputs, attns = model(img, return_attn=True)\n",
        "\n",
        "fig, ax = plt.subplots(2, 10, figsize=(10, 2))\n",
        "for i in range(10):\n",
        "    flattened_attns = (\n",
        "        attns.flatten(1, 2)[:, :, 0, 1:].mean(1).reshape(-1, 8, 8).cpu().numpy()\n",
        "    )\n",
        "    ax[0, i].imshow(inv_transform(img[i]))\n",
        "    ax[1, i].imshow(flattened_attns[i])\n",
        "    ax[0, i].axis(False)\n",
        "    ax[1, i].axis(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3096185a",
      "metadata": {
        "id": "3096185a"
      },
      "source": [
        "# Problem 4: Dialogue GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lqR5H5044Xvr",
      "metadata": {
        "id": "lqR5H5044Xvr"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a6fe8d",
      "metadata": {
        "id": "13a6fe8d"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"input.txt\"):\n",
        "    wget.download(\n",
        "        \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9133dc",
      "metadata": {
        "id": "8c9133dc"
      },
      "outputs": [],
      "source": [
        "with open(\"input.txt\", \"r\") as f:\n",
        "    raw_text = f.read()\n",
        "all_dialogues = raw_text.split(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2d6861",
      "metadata": {
        "id": "8e2d6861"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb804ff",
      "metadata": {
        "id": "ccb804ff"
      },
      "source": [
        "## Part 4.A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f26a57",
      "metadata": {
        "id": "20f26a57"
      },
      "outputs": [],
      "source": [
        "def tokenize(s):\n",
        "    return word_tokenize(s)\n",
        "\n",
        "\n",
        "class MyTokenizer:\n",
        "    def __init__(self, raw_text: str):\n",
        "        # raw_text     contains the text from which we will build our vocabulary\n",
        "\n",
        "        self.start = \"<START>\"  # token that starts every example\n",
        "        self.pad = \"<PAD>\"  # token used to pad examples to the same length\n",
        "        self.unk = \"<UNK>\"  # token used if encountering a word not in our vocabulary\n",
        "\n",
        "        vocab = np.unique(tokenize(raw_text))\n",
        "        vocab = np.concatenate([np.array([self.start, self.pad, self.unk]), vocab])\n",
        "\n",
        "        self.vocab = vocab  # array of tokens in order\n",
        "        self.tok_to_id = {w: i for i, w in enumerate(vocab)}  # mapping of token to ID\n",
        "        self.vocab_size = len(self.vocab)  # size of vocabulary\n",
        "\n",
        "    def encode(self, s: str) -> torch.Tensor:\n",
        "        # s           input string\n",
        "        #\n",
        "        # Output\n",
        "        # id_tensor   a tensor of token ids, starting with the start token.t\n",
        "\n",
        "        id_tensor = None\n",
        "\n",
        "        # TODO: tokenize the input using word_tokenize. Return a tensor\n",
        "        # of the token ids, starting with the token id for the start token.\n",
        "        # ============ ANSWER START ===========\n",
        "\n",
        "        # ============ ANSWER END =============\n",
        "\n",
        "        return id_tensor\n",
        "\n",
        "    def decode(self, toks: torch.Tensor) -> str:\n",
        "        # toks         a list of token ids\n",
        "        #\n",
        "        # Output\n",
        "        # decoded_str  the token ids decoded back into a string (join with a space)\n",
        "\n",
        "        decoded_str = None\n",
        "\n",
        "        # TODO: convert the token ids back to the actual corresponding words.\n",
        "        # Join the tokens with a space and return the full string\n",
        "        # ============ ANSWER START ===========\n",
        "\n",
        "        # ============ ANSWER END =============\n",
        "\n",
        "        return decoded_str\n",
        "\n",
        "    def pad_examples(self, tok_list: List[torch.Tensor]) -> torch.Tensor:\n",
        "        # Pads the tensors to the right with the pad token so that they are the same length.\n",
        "        #\n",
        "        # tok_list       a list of tensors containing token ids (maybe of different lengths)\n",
        "        #\n",
        "        # Output\n",
        "        # padded_tokens  shape: (len(tok_list), max length within tok_list)\n",
        "        return torch.nn.utils.rnn.pad_sequence(\n",
        "            tok_list, batch_first=True, padding_value=self.tok_to_id[self.pad]\n",
        "        )\n",
        "\n",
        "\n",
        "tok = MyTokenizer(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "328863ef",
      "metadata": {
        "id": "328863ef"
      },
      "outputs": [],
      "source": [
        "# tokenizer test cases\n",
        "input_string = \"KING RICHARD III:\\nSay that I did all this for love of her.\"\n",
        "enc = tok.encode(input_string)\n",
        "print(enc)\n",
        "dec = tok.decode(enc)\n",
        "print(dec)\n",
        "assert dec == \"<START> KING RICHARD III : Say that I did all this for love of her .\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3359721d",
      "metadata": {
        "id": "3359721d"
      },
      "source": [
        "# Part 4.B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f07f0a",
      "metadata": {
        "id": "60f07f0a"
      },
      "outputs": [],
      "source": [
        "class DialogueDataset:\n",
        "    def __init__(self, tokenizer: MyTokenizer, lines: List[str], max_N: int):\n",
        "        # tokenizer    an instance of MyTokenizer\n",
        "        # lines        a list of strings. each element in an example in the dataset\n",
        "        # max_N        the maximum number of tokens allowed per example. More than this will be truncated\n",
        "        self.lines = lines\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_N = max_N\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        # returns the example at int encoded by the tokenizer\n",
        "        # truncates the example if it is more than max_N tokens\n",
        "        return self.tokenizer.encode(self.lines[idx])[: self.max_N]\n",
        "\n",
        "\n",
        "def collate_fn(examples: List[torch.Tensor]):\n",
        "    # examples        a batch of tensors containing token ids (maybe of different lengths)\n",
        "    # Outputs a dictionary containing\n",
        "    #   input_ids     a single tensor with all of the examples padded (from the right) to the max\n",
        "    #                 length within the batch. shape:(B, max length within examples)\n",
        "    #   input_mask    a tensor indicating which tokens are padding and should be ignored. 0 if padding\n",
        "    #                 and 1 if not. shape: (B, max length within examples)\n",
        "    new_input_ids = tok.pad_examples(examples)\n",
        "    attn_mask = torch.ones(new_input_ids.shape)\n",
        "    attn_mask[new_input_ids == tok.tok_to_id[tok.pad]] = 0\n",
        "    return {\"input_ids\": tok.pad_examples(examples), \"input_mask\": attn_mask}\n",
        "\n",
        "\n",
        "ds = DialogueDataset(tok, all_dialogues, max_N=200)\n",
        "training_dl = torch.utils.data.DataLoader(ds, batch_size=64, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930c4fde",
      "metadata": {
        "id": "930c4fde"
      },
      "outputs": [],
      "source": [
        "# take a look at an example of an element from the training dataloader\n",
        "for batch in training_dl:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f980fc7d",
      "metadata": {
        "id": "f980fc7d"
      },
      "source": [
        "## Part 4.C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D3KyNSu_aKqC",
      "metadata": {
        "id": "D3KyNSu_aKqC"
      },
      "outputs": [],
      "source": [
        "embs = torch.ones((32, 100, 128))\n",
        "B, T, _ = embs.shape\n",
        "pos_ids = torch.arange(T).expand(B, -1)\n",
        "print(pos_ids)\n",
        "pos_E = nn.Embedding(200, 128)\n",
        "print(pos_E)\n",
        "pos_E(pos_ids).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f37a893a",
      "metadata": {
        "id": "f37a893a"
      },
      "outputs": [],
      "source": [
        "class DialogueGPT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        max_N: int,\n",
        "        dim: int,\n",
        "        attn_dim: int,\n",
        "        mlp_dim: int,\n",
        "        num_heads: int,\n",
        "        num_layers: int,\n",
        "    ):\n",
        "        # vocab_size       size of the vocabulary\n",
        "        # max_N            maximum number of tokens allowed to appear in 1 example\n",
        "        # dim              embedding dimension\n",
        "        # attn_dim         the hidden dimension of the attention layer\n",
        "        # mlp_dim          the hidden layer dimension of the FFN\n",
        "        # num_heads        the number of heads in the attention layer\n",
        "        # num_layers       the number of attention layers.\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: set up the token embedding and positional embeddings\n",
        "        #       Hint, use nn.Embedding\n",
        "        # ============ ANSWER START ============\n",
        "\n",
        "        # ============ ANSWER END ==============\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            dim=dim,\n",
        "            attn_dim=attn_dim,\n",
        "            mlp_dim=mlp_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, vocab_size))\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids: torch.Tensor, return_attn=False\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        # input_ids     a batch of input ids (right padded). shape: (B x T)\n",
        "        # return_attn   whether to return the attention weights\n",
        "        #\n",
        "        # Output\n",
        "        # out           the logit vector (B x T x V)\n",
        "        # alphas        the attention weights if return_attn is True. Otherwise None shape: (B, num_layers, num_heads, T, T)\n",
        "\n",
        "        embs = None\n",
        "\n",
        "        # TODO: retrieve the token embeddings for the input_ids.\n",
        "        #       Add to the token embeddings the positional embeddings.\n",
        "        #       Store the combined embedding in embs\n",
        "        # ============ ANSWER START ============\n",
        "\n",
        "        # ============ ANSWER END ============\n",
        "\n",
        "        causal_attn_mask = None\n",
        "\n",
        "        # TODO: Create the causal attention mask, which should be of size (B, T, T)\n",
        "        #       Remember that the causal attention mask is lower triangular (all tokens only\n",
        "        #       depend on themselves and the tokens before them).\n",
        "        # .      Store the mask in causal_attn_mask\n",
        "        # Hint: check out torch.tril\n",
        "        # ============ ANSWER START ============\n",
        "\n",
        "        # ============ ANSWER END ==============\n",
        "\n",
        "        x, alphas = self.transformer(\n",
        "            embs, attn_mask=causal_attn_mask, return_attn=return_attn\n",
        "        )\n",
        "        out = self.head(x)\n",
        "        return out, alphas\n",
        "\n",
        "    def generate(self, input_ids, num_tokens):\n",
        "        # you can assume batch size 1\n",
        "        with torch.no_grad():\n",
        "            for i in range(num_tokens):\n",
        "                out, _ = self.forward(input_ids)\n",
        "                new_token = torch.argmax(out[:, [-1]], -1)\n",
        "                input_ids = torch.cat([input_ids, new_token], dim=1)\n",
        "        return input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8b5f097",
      "metadata": {
        "id": "a8b5f097"
      },
      "source": [
        "## Part 4.D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a91f3300",
      "metadata": {
        "id": "a91f3300"
      },
      "outputs": [],
      "source": [
        "class DialogueLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\n",
        "    def forward(\n",
        "        self, logits: torch.Tensor, input_ids: torch.Tensor, inp_mask: torch.Tensor\n",
        "    ):\n",
        "        # logits      the logits produced by DialogueGPT. shape: (B x T x V)\n",
        "        # input_ids   the token ids. shape: (B x T)\n",
        "        # inp_mask    a 0/1 mask of which tokens are padding tokens and should be ignored. shape: (B x T)\n",
        "\n",
        "        # TODO: Implement the language model loss. For logits[i], we want to supervise the i+1 token_id\n",
        "        # with the cross entropy loss. We thus will not supervise the start token (input_ids[0]) or use\n",
        "        # the last logit vector (logits[-1]). Return the average of the losses for each token in the batch,\n",
        "        # making sure to ignore tokens corresponding to the padding (use inp_mask).\n",
        "\n",
        "        # ============ ANSWER START ============\n",
        "\n",
        "        # ============ ANSWER END ==============\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa39592a",
      "metadata": {
        "id": "fa39592a"
      },
      "source": [
        "## Part 4.F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e5b68b",
      "metadata": {
        "id": "06e5b68b"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = DialogueGPT(\n",
        "    vocab_size=tok.vocab_size,\n",
        "    max_N=200,\n",
        "    dim=128,\n",
        "    attn_dim=64,\n",
        "    mlp_dim=128,\n",
        "    num_heads=3,\n",
        "    num_layers=6,\n",
        ").cuda()\n",
        "criterion = DialogueLoss()\n",
        "\n",
        "NUM_EPOCHS = 80\n",
        "\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(), lr=0.0001, weight_decay=0\n",
        ")  # implement in homework\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c72fb2",
      "metadata": {
        "id": "63c72fb2"
      },
      "outputs": [],
      "source": [
        "# Time estimate: around 30 minutes on T4 GPU\n",
        "# Training\n",
        "import tqdm\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):  # loop over the dataset multiple times\n",
        "    loss_meter = AverageMeter()\n",
        "    for inp_dict in tqdm.tqdm(training_dl):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inp_ids, inp_mask = inp_dict[\"input_ids\"], inp_dict[\"input_mask\"]\n",
        "        inp_ids = inp_ids.cuda()\n",
        "        inp_mask = inp_mask.cuda()\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs, _ = model(input_ids=inp_ids)\n",
        "        loss = criterion(outputs, inp_ids, inp_mask)\n",
        "        loss_meter.update(loss.item(), len(inp_dict[\"input_ids\"]))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # print example\n",
        "    inp = tok.encode(\"\").unsqueeze(0).cuda()\n",
        "    print(tok.decode(model.generate(inp, 10)[0].cpu()))\n",
        "\n",
        "    print(\n",
        "        f\"Train Epoch: {epoch}, Loss: {loss_meter.calculate():0.4f}, LR: {scheduler.get_last_lr()[0]}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a66ac1b1",
      "metadata": {
        "id": "a66ac1b1"
      },
      "source": [
        "## Part 4.G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5d1381",
      "metadata": {
        "id": "2c5d1381"
      },
      "outputs": [],
      "source": [
        "inp = tok.encode(\"\").unsqueeze(0).cuda()\n",
        "print(tok.decode(model.generate(inp, 50)[0].cpu()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-401s9lBpzqe",
      "metadata": {
        "id": "-401s9lBpzqe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl_psets",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
