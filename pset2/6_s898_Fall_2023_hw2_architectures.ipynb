{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp7RKvwbwbpa"
      },
      "source": [
        "**NOTE:** For all questions in this notebook, include your answers in the write pdf submission. Do **not** submit the colab.\n",
        "\n",
        "# Homework 2\n",
        "\n",
        "We will use the CIFAR-100 in this notebook, which has **100** classes. Unlike homework 1, we will instead use CNN models to investigate some interesting aspects of training deep models. Moreover, we will use standard PyTorch `nn.Module`s and `optim.Optimizer`s.\n",
        "\n",
        "<h2>CIFAR-100</h2>\n",
        "\n",
        "CIFAR-100  is a image classification dataset.\n",
        "+ Each data sample is an RGB $32\\times32$ real image. A raw loaded image $\\in \\mathbb{R}^{3 \\times 32 \\times 32}$.\n",
        "+ Each image is associated with a label $\\in \\{0,1,2,\\dots, 99\\}$.\n",
        "\n",
        "\n",
        "Our goal is to train a neural network classifier that takes such $3\\times32\\times32$ images and predict a label $\\in \\{0, 1, 2, \\dots, 99\\}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MEnmRQuzLFj"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alPW6hlwE71a"
      },
      "source": [
        "You should run on GPU-enabled colab server (should be default for this notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OYaubOb0E6u7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from typing import *\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import dataclasses\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "assert torch.cuda.is_available(), \"Should use GPU-enabled colab\"\n",
        "\n",
        "device = torch.device('cuda:0')  # we will train with CUDA!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9zPaxSdbCjL"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "Let's reuse the homework 1 augmented dataset code, but now modified to work on **CIFAR-100**. Code is hidden in the cell below, but make sure to still run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uQIC0l06bCRt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169001437/169001437 [00:09<00:00, 17677992.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Training set size: 50000\n",
            "Validation set size: 10000\n",
            "CIFAR-100 classes: ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title\n",
        "%matplotlib inline\n",
        "\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def get_datasets(train_transforms=(), val_transforms=()):\n",
        "    r\"\"\"\n",
        "    Returns the CIFAR-100 training and validation datasets with corresponding\n",
        "    transforms.\n",
        "\n",
        "    `*_transforms` represent optional transformations, e.g., conversion to\n",
        "    PyTorch tensors, preprocessing, etc.\n",
        "    \"\"\"\n",
        "    train_set = torchvision.datasets.CIFAR100(\n",
        "        './data', train=True, download=True,\n",
        "        transform=torchvision.transforms.Compose(train_transforms))\n",
        "    val_set = torchvision.datasets.CIFAR100(\n",
        "        './data', train=False, download=True,\n",
        "        transform=torchvision.transforms.Compose(val_transforms))\n",
        "    return train_set, val_set\n",
        "\n",
        "\n",
        "cifar100_mean = torch.as_tensor([0.5071, 0.4865, 0.4409])\n",
        "cifar100_std = torch.as_tensor([0.2673, 0.2564, 0.2762])\n",
        "\n",
        "train_transforms = [\n",
        "    torchvision.transforms.RandomCrop(32, padding=3, padding_mode='reflect'),\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=cifar100_mean, std=cifar100_std),\n",
        "]\n",
        "\n",
        "val_transforms = [\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=cifar100_mean, std=cifar100_std),\n",
        "]\n",
        "\n",
        "train_set, val_set = get_datasets(train_transforms, val_transforms)\n",
        "\n",
        "print(f\"Training set size: {len(train_set)}\")\n",
        "print(f\"Validation set size: {len(val_set)}\")\n",
        "\n",
        "class_names = train_set.classes\n",
        "print(f'CIFAR-100 classes: {class_names}')\n",
        "\n",
        "def visualize_tensor_data(data: torch.Tensor, label: int):\n",
        "    # Data is a tensor of shape [C, W, H]  (C is the channel dimension, 3 for RGB)\n",
        "    # Put channel at last\n",
        "    data = data.permute(1, 2, 0)\n",
        "    # Un-normalize\n",
        "    data = data * cifar100_std + cifar100_mean\n",
        "\n",
        "    plt.imshow(data)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Label = {class_names[label]}')\n",
        "\n",
        "data, label = train_set[13]\n",
        "visualize_tensor_data(data, label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6SOqXmUzLg9"
      },
      "source": [
        "## Network Definition\n",
        "\n",
        "**Question 5**: Implement the following function that creates the specified CNN networks. Read through the docstring and make sure that your implementation behaves as required. Afterwards, print the outputs of\n",
        "+ `make_cnn(100, 'tanh', num_conv_layers=5, num_fc_layers=4)`\n",
        "+ `make_cnn(3, 'none', num_conv_layers=3, num_fc_layers=2)`\n",
        "\n",
        "Add both your implementation and outputs to the writeup.\n",
        "\n",
        "---\n",
        "\n",
        "Links you may find useful:\n",
        "+ `none` activation: https://pytorch.org/docs/stable/generated/torch.nn.Identity.html?highlight=identity#torch.nn.Identity\n",
        "+ Chaining layers: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html?highlight=sequential#torch.nn.Sequential\n",
        "+ Reshaping image-like tensors to vectors: https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html?highlight=flatten#torch.nn.Flatten\n",
        "\n",
        "(You are not required to use these classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PKk9djqb6Fc0"
      },
      "outputs": [],
      "source": [
        "def make_mlp(\n",
        "    num_inputs: int, num_outputs: int, activation: str, hidden_sizes: int = [128]\n",
        ") -> nn.Module:\n",
        "\n",
        "\n",
        "    if activation == \"relu\":\n",
        "\n",
        "        act_cls = nn.ReLU\n",
        "\n",
        "    elif activation == \"tanh\":\n",
        "\n",
        "        act_cls = nn.Tanh\n",
        "\n",
        "    elif activation == \"none\":\n",
        "\n",
        "        act_cls = nn.Identity\n",
        "    else:\n",
        "\n",
        "\n",
        "        raise ValueError(f\"Unexpected activation={repr(activation)}\")\n",
        "\n",
        "\n",
        "    net = [nn.Flatten(), nn.Linear(num_inputs, hidden_sizes[0]), act_cls()]\n",
        "\n",
        "\n",
        "    for i in range(1, len(hidden_sizes)):\n",
        "\n",
        "        net.extend([nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]), act_cls()])\n",
        "\n",
        "    net.extend([nn.Linear(hidden_sizes[-1], num_outputs)])\n",
        "\n",
        "\n",
        "    return nn.Sequential(*net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nPEYe7bbwbpd"
      },
      "outputs": [],
      "source": [
        "def make_cnn(\n",
        "    num_outputs: int, activation: str, num_conv_layers: int = 4, num_fc_layers: int = 2\n",
        ") -> nn.Module:\n",
        "    r\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Returns a PyTorch module representing a CNN network that takes in image-like input with shape [3, 32, 32].\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Args:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        num_outputs (int): size of the final output layer. E.g., if the network is a classifier, this is usually #classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        activation (str): activation functions between conv/linear layers. Supported choices are ['relu', 'tanh', 'none'] (always strings).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            For 'none', no activation function is applied, and the previous conv/linear output is directly fed into the next conv/linear.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        num_conv_layers (int): number of conv layers in CNN. This should be >= 3 and <= 5.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            First conv layer should always use 5x5 kernels, mapping 3-channel data to 12-channel data, with **reflect** padding=2 and no striding.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            Subsequent conv layers should always use 3x3 kernels, mapping to 64-channel data, with zeros (default) padding=1 and strides=2.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        num_fc_layers (int): number of fc/linear layers after the convolutional part. This should be >= 2.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            For all but the last fc layer, it should output 128-dimensional vectors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Returns:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        (nn.Module) The CNN network of desired tructure.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    If an unexpected input is given, this will raise a ValueError. (You can assume that arguments are of correct types.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # FIXME\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        act_cls = nn.ReLU\n",
        "    elif activation == \"tanh\":\n",
        "        act_cls = nn.Tanh\n",
        "    elif activation == \"none\":\n",
        "        act_cls = nn.Identity\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected activation={repr(activation)}\")\n",
        "    net = [\n",
        "        nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=12,\n",
        "            kernel_size=5,\n",
        "            padding_mode=\"reflect\",\n",
        "            padding=2,\n",
        "            stride=1,\n",
        "        ),\n",
        "        act_cls(),\n",
        "    ]\n",
        "    num_out = 12\n",
        "    for _ in range(num_conv_layers - 1):\n",
        "        net.extend(\n",
        "            [\n",
        "                nn.Conv2d(\n",
        "                    in_channels=num_out,\n",
        "                    out_channels=64,\n",
        "                    kernel_size=3,\n",
        "                    padding=1,\n",
        "                    stride=2,\n",
        "                ),\n",
        "                act_cls(),\n",
        "            ]\n",
        "        )\n",
        "        num_out = 64\n",
        "    net.extend([nn.Flatten()])\n",
        "    for _ in range(num_fc_layers - 1):\n",
        "        net.extend([nn.LazyLinear(out_features=128), act_cls()])\n",
        "    net.extend([nn.LazyLinear(out_features=num_outputs)])\n",
        "    return nn.Sequential(*net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j6w5mydqZpN6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_outputs=100, tanh, hidden_sizes=[128, 120]\n",
            "Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Linear(in_features=10, out_features=128, bias=True)\n",
            "  (2): Tanh()\n",
            "  (3): Linear(in_features=128, out_features=120, bias=True)\n",
            "  (4): Tanh()\n",
            "  (5): Linear(in_features=120, out_features=100, bias=True)\n",
            ")\n",
            "\n",
            "num_outputs=100, tanh, num_conv_layers=5, num_fc_layers=4:\n",
            "Sequential(\n",
            "  (0): Conv2d(3, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=reflect)\n",
            "  (1): Tanh()\n",
            "  (2): Conv2d(12, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (3): Tanh()\n",
            "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (5): Tanh()\n",
            "  (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (7): Tanh()\n",
            "  (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (9): Tanh()\n",
            "  (10): Flatten(start_dim=1, end_dim=-1)\n",
            "  (11): LazyLinear(in_features=0, out_features=128, bias=True)\n",
            "  (12): Tanh()\n",
            "  (13): LazyLinear(in_features=0, out_features=128, bias=True)\n",
            "  (14): Tanh()\n",
            "  (15): LazyLinear(in_features=0, out_features=128, bias=True)\n",
            "  (16): Tanh()\n",
            "  (17): LazyLinear(in_features=0, out_features=100, bias=True)\n",
            ")\n",
            "\n",
            "num_outputs=3, no activation, num_conv_layers=3, num_fc_layers=2:\n",
            "Sequential(\n",
            "  (0): Conv2d(3, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=reflect)\n",
            "  (1): Identity()\n",
            "  (2): Conv2d(12, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (3): Identity()\n",
            "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (5): Identity()\n",
            "  (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  (7): LazyLinear(in_features=0, out_features=128, bias=True)\n",
            "  (8): Identity()\n",
            "  (9): LazyLinear(in_features=0, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(\"num_outputs=100, tanh, hidden_sizes=[128, 120]\")\n",
        "print(make_mlp(10, 100, \"tanh\", hidden_sizes=[128, 120]))\n",
        "print()\n",
        "print(\"num_outputs=100, tanh, num_conv_layers=5, num_fc_layers=4:\")\n",
        "print(make_cnn(100, \"tanh\", num_conv_layers=5, num_fc_layers=4))\n",
        "print()\n",
        "print(\"num_outputs=3, no activation, num_conv_layers=3, num_fc_layers=2:\")\n",
        "print(make_cnn(3, \"none\", num_conv_layers=3, num_fc_layers=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAW_ElprIlZd"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "**Question 6**: Implement the following training and evaluate functions according to docstring. Complete the `FIXME`s in `train_epoch`, `evaluate`, and `train`. Attach your code to PDF.\n",
        "\n",
        "---\n",
        "\n",
        "Links you may find useful:\n",
        "+ Classification loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html?highlight=cross_entropy#torch.nn.functional.cross_entropy\n",
        "+ Learning rate adjustment: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "\n",
        "(You are not required to use these classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TyU21dzCIzGS"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    epoch: int,\n",
        "    model: nn.Module,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    optim: torch.optim.Optimizer,\n",
        "):\n",
        "    r\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Trains `model` on `train_loader` for cross entropy loss for one epoch.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Args:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        epoch (int): the current epoch number (i.e., number of epochs done before this one).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        model (nn.Module): our network (created using `make_cnn`).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        train_loader (DataLoader): a DataLoader that yields (batched_images, batched_target_class_indices) when iterated.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        optim (Optimizer): optimizer object that is created with `model` parameters and should be used for updating `model` in this function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Returns:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        (List[float]) the losses computed at each iteration as a list of *Python* numbers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    NOTE:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      + Remember to clear previously computed gradient at beginning of each iteration.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      + Convert loss values to python floats (e.g., via `.item()`) before adding to `loss_values`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    loss_values: List[float] = []\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for data, target in tqdm(train_loader, desc=f\"Training @ epoch {epoch}\"):\n",
        "        optim.zero_grad()\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        # make predictions\n",
        "        prediction = model(data)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_fn(input=prediction, target=target)\n",
        "        loss_values.append(loss.item())\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "    return loss_values\n",
        "\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class EvaluateResult:\n",
        "    r\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    A collection containing everything we need to know about the evaluate results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    See `evaluate` docstring for meanings of the members of this class\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    acc: float  # overall accuracy\n",
        "\n",
        "    correct_predictions: torch.Tensor  # size |dataset|\n",
        "\n",
        "    confidence: torch.Tensor  # size |dataset|\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, loader: torch.utils.data.DataLoader) -> EvaluateResult:\n",
        "    r\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    Evaluate a classifier `model` on dataset contained in `loader`.\n",
        "\n",
        "\n",
        "\n",
        "    For each input, the predicted label is taken as one with highest probability in the distribution given by `model`.\n",
        "\n",
        "\n",
        "\n",
        "    In addition to compute overall accuracies, we also output\n",
        "\n",
        "\n",
        "\n",
        "        (1) a boolean Tensor with size |dataset|, showing whether each sample is correctly classified.\n",
        "\n",
        "\n",
        "\n",
        "        (2) a float32 Tensor with size |dataset|, showing the `model`'s assigned probability for its prediction, called *confidence*.\n",
        "\n",
        "\n",
        "\n",
        "    Args:\n",
        "\n",
        "\n",
        "\n",
        "        model (nn.Module): our network (created using `make_cnn`).\n",
        "\n",
        "\n",
        "\n",
        "        loader (DataLoader): a DataLoader that yields (batched_images, batched_target_class_indices) when iterated.\n",
        "\n",
        "\n",
        "\n",
        "    Returns:\n",
        "\n",
        "\n",
        "\n",
        "        (EvaluateResult) Containing overall accuracy, whether each sample is correctly classified, and confidence.\n",
        "\n",
        "\n",
        "\n",
        "            The tensors should be on **CPU**.\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # FIXME\n",
        "\n",
        "    correct_predictions = []\n",
        "    confidence = []\n",
        "    model.eval()\n",
        "    for data, target in loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = model(data)\n",
        "        prob = torch.softmax(output, dim=1)\n",
        "        pred = torch.argmax(prob, dim=1)\n",
        "        conf = torch.max(prob, dim=1).values\n",
        "        correct_predictions.append((pred == target).detach().cpu())\n",
        "        confidence.append(conf.detach().cpu())\n",
        "\n",
        "    correct_predictions = torch.cat(correct_predictions).cpu()\n",
        "    confidence = torch.cat(confidence).cpu()\n",
        "    acc = correct_predictions.float().mean().item()\n",
        "    eval_result = EvaluateResult(acc, correct_predictions, confidence)\n",
        "    return eval_result\n",
        "\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class TrainResult:\n",
        "    r\"\"\"\n",
        "\n",
        "\n",
        "    A collection containing everything we need to know about the training results\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    num_epochs: int\n",
        "\n",
        "    # Training loss (saved at each iteration in `train_epoch`)\n",
        "    train_losses: List[float]\n",
        "\n",
        "    # The epochs where we perform evaluation\n",
        "    eval_epochs: List[int]\n",
        "\n",
        "    # Training accuracies, computed at each epoch in `eval_epochs`\n",
        "    train_accs: List[float]\n",
        "\n",
        "    # Validation accuracies, computed at each epoch in `eval_epochs`\n",
        "    val_accs: List[float]\n",
        "\n",
        "    # The last validation evaluation full result\n",
        "\n",
        "    final_val_eval_result: EvaluateResult = None\n",
        "\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_set,\n",
        "    val_set,\n",
        "    *,\n",
        "    num_epochs=60,\n",
        "    lr=0.003,\n",
        "    train_epoch_fn=train_epoch,\n",
        "    **kwargs,\n",
        "):\n",
        "    r\"\"\"\n",
        "\n",
        "\n",
        "    Train `model` on `train_set` for `num_epochs` epochs using **Adam** optimizer and `lr` learning rate\n",
        "\n",
        "\n",
        "    following a decay schedule by a factor of `0.3` at epochs `[5, 50]`.\n",
        "\n",
        "\n",
        "    Args:\n",
        "\n",
        "\n",
        "        model (nn.Module): our network (created using `make_cnn`).\n",
        "\n",
        "\n",
        "        train_set (Dataset): CIFAR-100 training dataset.\n",
        "\n",
        "\n",
        "        val_set (Dataset): CIFAR-100 validation dataset. Evaluated every *5* epochs and at the end of training.\n",
        "\n",
        "\n",
        "        num_epochs (int): number of total training epochs.\n",
        "\n",
        "\n",
        "        lr (float): initial learning rate.\n",
        "\n",
        "\n",
        "        train_epoch_fn (Callable): function that trains the model for a single epoch. This is `train_epoch`\n",
        "\n",
        "            usually, but we will use different choices in later questions.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "\n",
        "\n",
        "        (TrainResult)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Data loaders\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set, batch_size=512, shuffle=True\n",
        "    )  # Random order for training (\"[S]tochastic\" in SGD)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=1024, shuffle=False)\n",
        "\n",
        "\n",
        "    # Our classifier\n",
        "\n",
        "    print(\"Model =\", model)\n",
        "\n",
        "\n",
        "    # Create optimizer and lr scheduler\n",
        "\n",
        "    # FIXME\n",
        "\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "        optim, milestones=[5, 50], gamma=0.3\n",
        "    )\n",
        "\n",
        "\n",
        "    result: TrainResult = TrainResult(\n",
        "        num_epochs, train_losses=[], eval_epochs=[], train_accs=[], val_accs=[]\n",
        "    )\n",
        "\n",
        "    last_eval_epoch = -float(\"inf\")\n",
        "\n",
        "\n",
        "    # Iterate through the entire training dataset `num_epochs` times\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        if epoch - last_eval_epoch >= 5:\n",
        "\n",
        "            result.eval_epochs.append(epoch)\n",
        "\n",
        "            result.train_accs.append(evaluate(model, train_loader).acc)\n",
        "\n",
        "            result.val_accs.append(evaluate(model, val_loader).acc)\n",
        "\n",
        "            print(\n",
        "                f\"Epoch = {epoch:> 2d}    Train acc = {result.train_accs[-1]:.2%}    Val acc = {result.val_accs[-1]:.2%}\"\n",
        "            )\n",
        "\n",
        "            last_eval_epoch = epoch\n",
        "\n",
        "        # Train over the entire `train_set` with given `train_epoch_fn` function (i.e., one epoch)\n",
        "\n",
        "        result.train_losses.extend(\n",
        "            train_epoch_fn(epoch, model, train_loader, optim, **kwargs)\n",
        "        )\n",
        "\n",
        "        # Evaluate with our `evaluate` function\n",
        "\n",
        "        print(f\"Epoch = {epoch:> 2d}    Train loss = {result.train_losses[-1]:.4f}\")\n",
        "\n",
        "\n",
        "        # Adjust learning rate if needed\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    result.eval_epochs.append(num_epochs)\n",
        "\n",
        "    result.train_accs.append(evaluate(model, train_loader).acc)\n",
        "\n",
        "    result.final_val_eval_result = evaluate(model, val_loader)\n",
        "\n",
        "    result.val_accs.append(result.final_val_eval_result.acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch = {num_epochs:> 2d}    Train acc = {result.train_accs[-1]:.2%}    Val acc = {result.val_accs[-1]:.2%}\"\n",
        "    )\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16idc1KatEsD"
      },
      "source": [
        "We provide the following learning curve plotting function (similar to homework 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ioPuiGngtEMO"
      },
      "outputs": [],
      "source": [
        "def learning_curve(result: TrainResult, *, title: str = \"Learning Curve\"):\n",
        "    r\"\"\"\n",
        "    Plot the training loss, training accuracy, and validation accuracy versus\n",
        "    epochs taken.\n",
        "    \"\"\"\n",
        "    fig, ax_loss = plt.subplots(figsize=(8, 5))\n",
        "    ax_loss.set_title(title, fontsize=16)\n",
        "    ax_loss.set_xlabel(\"Epoch\", fontsize=12)\n",
        "\n",
        "    l_trloss = ax_loss.plot(\n",
        "        torch.arange(len(result.train_losses))\n",
        "        / len(result.train_losses)\n",
        "        * result.num_epochs,\n",
        "        result.train_losses,\n",
        "        label=\"Train loss\",\n",
        "        color=\"C0\",\n",
        "    )\n",
        "    ax_loss.set_ylim(0, 4.8)\n",
        "    ax_loss.set_ylabel(\"Train loss\", color=\"C0\", fontsize=12)\n",
        "    ax_loss.tick_params(axis=\"y\", labelcolor=\"C0\")\n",
        "\n",
        "    ax_acc = ax_loss.twinx()\n",
        "    l_tracc = ax_acc.plot(\n",
        "        result.eval_epochs,\n",
        "        result.train_accs,\n",
        "        label=\"Train acc\",\n",
        "        color=\"C1\",\n",
        "        linestyle=\"--\",\n",
        "    )\n",
        "    if len(result.val_accs):\n",
        "        l_valacc = ax_acc.plot(\n",
        "            result.eval_epochs, result.val_accs, label=\"Val acc\", color=\"C1\"\n",
        "        )\n",
        "    else:\n",
        "        l_valacc = []\n",
        "    ax_acc.set_ylim(0, 1)\n",
        "    ax_acc.set_ylabel(\"Accuracies\", color=\"C1\", fontsize=12)\n",
        "    ax_acc.tick_params(axis=\"y\", labelcolor=\"C1\")\n",
        "\n",
        "    lines = l_trloss + l_tracc + l_valacc\n",
        "    ax_loss.legend(lines, [l.get_label() for l in lines], loc=\"upper left\", fontsize=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-IF50rjkL1ZF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3072 100\n",
            "Model = Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Linear(in_features=3072, out_features=128, bias=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (6): ReLU()\n",
            "  (7): Linear(in_features=128, out_features=100, bias=True)\n",
            ")\n",
            "Epoch =  0    Train acc = 0.84%    Val acc = 0.70%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 0: 100%|██████████| 98/98 [00:11<00:00,  8.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  0    Train loss = 4.6138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 1: 100%|██████████| 98/98 [00:11<00:00,  8.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  1    Train loss = 4.6193\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 2: 100%|██████████| 98/98 [00:11<00:00,  8.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  2    Train loss = 4.6171\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 3: 100%|██████████| 98/98 [00:11<00:00,  8.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  3    Train loss = 4.6199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 4: 100%|██████████| 98/98 [00:11<00:00,  8.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  4    Train loss = 4.6202\n",
            "Epoch =  5    Train acc = 1.00%    Val acc = 1.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 5: 100%|██████████| 98/98 [00:11<00:00,  8.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  5    Train loss = 4.6169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 6: 100%|██████████| 98/98 [00:11<00:00,  8.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  6    Train loss = 4.6175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 7: 100%|██████████| 98/98 [00:11<00:00,  8.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  7    Train loss = 4.5999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 8: 100%|██████████| 98/98 [00:11<00:00,  8.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  8    Train loss = 4.6161\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 9: 100%|██████████| 98/98 [00:11<00:00,  8.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  9    Train loss = 4.6131\n",
            "Epoch =  10    Train acc = 1.00%    Val acc = 1.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 10: 100%|██████████| 98/98 [00:11<00:00,  8.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  10    Train loss = 4.6149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 11: 100%|██████████| 98/98 [00:11<00:00,  8.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  11    Train loss = 4.6100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 12: 100%|██████████| 98/98 [00:11<00:00,  8.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  12    Train loss = 4.5982\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 13: 100%|██████████| 98/98 [00:11<00:00,  8.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  13    Train loss = 4.6067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 14: 100%|██████████| 98/98 [00:11<00:00,  8.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  14    Train loss = 4.6173\n",
            "Epoch =  15    Train acc = 1.00%    Val acc = 1.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 15: 100%|██████████| 98/98 [00:11<00:00,  8.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  15    Train loss = 4.6121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 16: 100%|██████████| 98/98 [00:10<00:00,  8.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  16    Train loss = 4.6160\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 17: 100%|██████████| 98/98 [00:11<00:00,  8.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  17    Train loss = 4.6105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 18: 100%|██████████| 98/98 [00:11<00:00,  8.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  18    Train loss = 4.6146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 19: 100%|██████████| 98/98 [00:11<00:00,  8.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  19    Train loss = 4.6082\n",
            "Epoch =  20    Train acc = 1.00%    Val acc = 1.00%\n"
          ]
        }
      ],
      "source": [
        "# Create a MLP 128 width, 3 depth on the GPU, train it, and plot learning curves.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "img_tensor_size = train_set[0][0].numel()\n",
        "class_num = len(train_set.classes)\n",
        "print(img_tensor_size, class_num)\n",
        "model_MLP = make_mlp(img_tensor_size, class_num, \"relu\", [128, 128, 128])\n",
        "model_MLP = model_MLP.to(device)\n",
        "result_mlp = train(model_MLP, train_set, val_set, num_epochs=20, lr=0.15)\n",
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRzxoA4uAYef"
      },
      "outputs": [],
      "source": [
        "# Create a MLP 128 width, 7 depth on the GPU, train it, and plot learning curves.\n",
        "model_MLP2 = make_mlp(\n",
        "    img_tensor_size, class_num, \"relu\", [128, 128, 128, 128, 128, 128, 128]\n",
        ")\n",
        "model_MLP2 = model_MLP2.to(device)\n",
        "result_mlp2 = train(model_MLP2, train_set, val_set, num_epochs=20, lr=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "906E2ktx1nfH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model = Sequential(\n",
            "  (0): Conv2d(3, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=reflect)\n",
            "  (1): ReLU()\n",
            "  (2): Conv2d(12, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (3): ReLU()\n",
            "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (5): ReLU()\n",
            "  (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (7): ReLU()\n",
            "  (8): Flatten(start_dim=1, end_dim=-1)\n",
            "  (9): LazyLinear(in_features=0, out_features=128, bias=True)\n",
            "  (10): ReLU()\n",
            "  (11): LazyLinear(in_features=0, out_features=128, bias=True)\n",
            "  (12): ReLU()\n",
            "  (13): LazyLinear(in_features=0, out_features=100, bias=True)\n",
            ")\n",
            "Epoch =  0    Train acc = 1.00%    Val acc = 1.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 0: 100%|██████████| 98/98 [00:12<00:00,  7.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  0    Train loss = 4.6131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 1: 100%|██████████| 98/98 [00:11<00:00,  8.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch =  1    Train loss = 4.6241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training @ epoch 2:  64%|██████▍   | 63/98 [00:07<00:04,  8.11it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model_CNN \u001b[38;5;241m=\u001b[39m make_cnn(class_num, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m model_CNN \u001b[38;5;241m=\u001b[39m model_CNN\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m result_cnn \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_CNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[18], line 973\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_set, val_set, num_epochs, lr, train_epoch_fn, **kwargs)\u001b[0m\n\u001b[0;32m    968\u001b[0m     last_eval_epoch \u001b[38;5;241m=\u001b[39m epoch\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# Train over the entire `train_set` with given `train_epoch_fn` function (i.e., one epoch)\u001b[39;00m\n\u001b[0;32m    972\u001b[0m result\u001b[38;5;241m.\u001b[39mtrain_losses\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m--> 973\u001b[0m     train_epoch_fn(epoch, model, train_loader, optim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    974\u001b[0m )\n\u001b[0;32m    976\u001b[0m \u001b[38;5;66;03m# Evaluate with our `evaluate` function\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m> 2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m    Train loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mtrain_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[18], line 317\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(epoch, model, train_loader, optim)\u001b[0m\n\u001b[0;32m    312\u001b[0m loss_values: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    314\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m--> 317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining @ epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    318\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    319\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\torchvision\\transforms\\transforms.py:681\u001b[0m, in \u001b[0;36mRandomCrop.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    678\u001b[0m     padding \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m height]\n\u001b[0;32m    679\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(img, padding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode)\n\u001b[1;32m--> 681\u001b[0m i, j, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcrop(img, i, j, h, w)\n",
            "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dl_psets\\lib\\site-packages\\torchvision\\transforms\\transforms.py:645\u001b[0m, in \u001b[0;36mRandomCrop.get_params\u001b[1;34m(img, output_size)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;241m==\u001b[39m tw \u001b[38;5;129;01mand\u001b[39;00m h \u001b[38;5;241m==\u001b[39m th:\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, h, w\n\u001b[1;32m--> 645\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    646\u001b[0m j \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, w \u001b[38;5;241m-\u001b[39m tw \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m i, j, th, tw\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Create a CNN 4 convolutional layers, 3 MLP layers on the GPU, train it, and plot learning curves.\n",
        "model_CNN = make_cnn(class_num, \"relu\", 4, 3)\n",
        "model_CNN = model_CNN.to(device)\n",
        "result_cnn = train(model_CNN, train_set, val_set, num_epochs=20, lr=0.15)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
